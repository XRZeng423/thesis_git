{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /software/python-anaconda-2020.11-el8-x86_64/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "#import librosa\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import ffprobe\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import wave\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "from teager_py import Teager\n",
    "import pydub\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# zip file handler  \n",
    "zip = zipfile.ZipFile('/home/xuranzeng/thesis/ReleasedDataset.zip')\n",
    "\n",
    "# list available files in the container\n",
    "filename = zip.namelist()\n",
    "\n",
    "# clean path and file names\n",
    "audioname = (item for item in filename if item.endswith('mp3')) \n",
    "data = pd.DataFrame(list(audioname))\n",
    "data = pd.concat([data, data[0].str.split('/', 4, expand=True)], axis=1) # split path \n",
    "data = pd.concat([data, data[1].str.split('_', 2, expand=True)], axis=1) # split company and day\n",
    "data.columns=[\"Path\",\"Basic\", \"Company_Day\", \"Type\", \"Filename\",\"Company\",\"Day\"] # rename column\n",
    "data = data[['Path','Company',\"Day\",\"Filename\"]] # delete useless columns\n",
    "\n",
    "# path\n",
    "base_path='/home/xuranzeng/thesis/EC/'\n",
    "data['Complete_Path'] = base_path + data['Path'].astype(str)\n",
    "data = data[['Complete_Path','Company',\"Day\",\"Filename\"]] # delete useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complete_Path</th>\n",
       "      <th>Company</th>\n",
       "      <th>Day</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_15_1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_16_1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_1_14.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_2_11.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_10_5.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_9_11.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_15_4.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_1_45.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_12_1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>20170509</td>\n",
       "      <td>Daniel J. Finnegan_6_13.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Complete_Path               Company  \\\n",
       "0  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "1  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "2  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "3  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "4  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "5  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "6  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "7  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "8  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "9  /home/xuranzeng/thesis/EC/ReleasedDataset_mp3/...  Booking Holdings Inc   \n",
       "\n",
       "        Day                     Filename  \n",
       "0  20170509  Daniel J. Finnegan_15_1.mp3  \n",
       "1  20170509  Daniel J. Finnegan_16_1.mp3  \n",
       "2  20170509  Daniel J. Finnegan_1_14.mp3  \n",
       "3  20170509  Daniel J. Finnegan_2_11.mp3  \n",
       "4  20170509  Daniel J. Finnegan_10_5.mp3  \n",
       "5  20170509  Daniel J. Finnegan_9_11.mp3  \n",
       "6  20170509  Daniel J. Finnegan_15_4.mp3  \n",
       "7  20170509  Daniel J. Finnegan_1_45.mp3  \n",
       "8  20170509  Daniel J. Finnegan_12_1.mp3  \n",
       "9  20170509  Daniel J. Finnegan_6_13.mp3  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_path=data[['Company','Day']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_path.to_csv('/home/xuranzeng/thesis/symbol_company_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_symbol= pd.read_csv('/home/xuranzeng/thesis/symbol_company_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Day</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>20170725</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>20170425</td>\n",
       "      <td>MMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A.O. Smith Corp</td>\n",
       "      <td>20170726</td>\n",
       "      <td>AOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>20171018</td>\n",
       "      <td>ABT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>20171027</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>566</td>\n",
       "      <td>Xerox</td>\n",
       "      <td>20170801</td>\n",
       "      <td>XRX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>567</td>\n",
       "      <td>Xilinx</td>\n",
       "      <td>20170426</td>\n",
       "      <td>XLNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>568</td>\n",
       "      <td>XL Group</td>\n",
       "      <td>20171024</td>\n",
       "      <td>XL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>569</td>\n",
       "      <td>Yum! Brands Inc</td>\n",
       "      <td>20170803</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>570</td>\n",
       "      <td>Yum! Brands Inc</td>\n",
       "      <td>20170209</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0              Company       Day Symbol\n",
       "0             0           3M Company  20170725    MMM\n",
       "1             1           3M Company  20170425    MMM\n",
       "2             2      A.O. Smith Corp  20170726    AOS\n",
       "3             3  Abbott Laboratories  20171018    ABT\n",
       "4             4          AbbVie Inc.  20171027   ABBV\n",
       "..          ...                  ...       ...    ...\n",
       "566         566                Xerox  20170801    XRX\n",
       "567         567               Xilinx  20170426   XLNX\n",
       "568         568             XL Group  20171024     XL\n",
       "569         569      Yum! Brands Inc  20170803    YUM\n",
       "570         570      Yum! Brands Inc  20170209    YUM\n",
       "\n",
       "[571 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('/home/xuranzeng/thesis/EC_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pd.read_csv('/home/xuranzeng/thesis/EC_path.csv')\n",
    "path['Day']=pd.to_numeric(path.Day)\n",
    "company_name = \"3M Company\"\n",
    "day_name = 20170425\n",
    "sample = path.loc[(path[\"Company\"]==company_name) & (path[\"Day\"]==day_name)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = \"3M Company\"\n",
    "day_name = 20170425\n",
    "sample = path.loc[(path[\"Company\"]==company_name) & (path[\"Day\"]==day_name)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"Complete_Path\"]]\n",
    "sample = sample.rename(columns={'Complete_Path': 'mp3path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.to_csv('/home/xuranzeng/TSFSER/code/data/sample_EC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeMAPS\n",
    "import opensmile\n",
    "\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,   \n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GeMAPS(path):\n",
    "    GeMAPS = []\n",
    "    for i in range(0,len(path)):\n",
    "        filepath = path.loc[i,'mp3path']\n",
    "        audio = AudioSegment.from_mp3(filepath)\n",
    "        sr = audio.frame_rate\n",
    "        audio = audio.get_array_of_samples()\n",
    "        df = smile.process_signal(audio,sr)\n",
    "        GeMAPS.append(pd.DataFrame(df.mean(axis=0)).T)\n",
    "        print('processing:',i)\n",
    "    \n",
    "\n",
    "    GeMAPS = pd.concat(GeMAPS).reset_index()\n",
    "    return GeMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 0\n",
      "processing: 1\n",
      "processing: 2\n",
      "processing: 3\n",
      "processing: 4\n",
      "processing: 5\n",
      "processing: 6\n",
      "processing: 7\n",
      "processing: 8\n",
      "processing: 9\n",
      "processing: 10\n",
      "processing: 11\n",
      "processing: 12\n",
      "processing: 13\n",
      "processing: 14\n",
      "processing: 15\n",
      "processing: 16\n",
      "processing: 17\n",
      "processing: 18\n",
      "processing: 19\n",
      "processing: 20\n",
      "processing: 21\n",
      "processing: 22\n",
      "processing: 23\n",
      "processing: 24\n",
      "processing: 25\n",
      "processing: 26\n",
      "processing: 27\n",
      "processing: 28\n",
      "processing: 29\n",
      "processing: 30\n",
      "processing: 31\n",
      "processing: 32\n",
      "processing: 33\n",
      "processing: 34\n",
      "processing: 35\n",
      "processing: 36\n",
      "processing: 37\n",
      "processing: 38\n",
      "processing: 39\n",
      "processing: 40\n",
      "processing: 41\n",
      "processing: 42\n",
      "processing: 43\n",
      "processing: 44\n",
      "processing: 45\n",
      "processing: 46\n",
      "processing: 47\n",
      "processing: 48\n",
      "processing: 49\n",
      "processing: 50\n",
      "processing: 51\n",
      "processing: 52\n",
      "processing: 53\n",
      "processing: 54\n",
      "processing: 55\n",
      "processing: 56\n",
      "processing: 57\n",
      "processing: 58\n",
      "processing: 59\n",
      "processing: 60\n",
      "processing: 61\n",
      "processing: 62\n",
      "processing: 63\n",
      "processing: 64\n",
      "processing: 65\n",
      "processing: 66\n",
      "processing: 67\n",
      "processing: 68\n",
      "processing: 69\n",
      "processing: 70\n",
      "processing: 71\n",
      "processing: 72\n",
      "processing: 73\n",
      "processing: 74\n",
      "processing: 75\n",
      "processing: 76\n",
      "processing: 77\n",
      "processing: 78\n",
      "processing: 79\n",
      "processing: 80\n",
      "processing: 81\n",
      "processing: 82\n",
      "processing: 83\n",
      "processing: 84\n",
      "processing: 85\n",
      "processing: 86\n",
      "processing: 87\n",
      "processing: 88\n",
      "processing: 89\n",
      "processing: 90\n",
      "processing: 91\n",
      "processing: 92\n",
      "processing: 93\n",
      "processing: 94\n",
      "processing: 95\n",
      "processing: 96\n",
      "processing: 97\n",
      "processing: 98\n",
      "processing: 99\n",
      "processing: 100\n",
      "processing: 101\n",
      "processing: 102\n",
      "processing: 103\n",
      "processing: 104\n",
      "processing: 105\n",
      "processing: 106\n",
      "processing: 107\n",
      "processing: 108\n",
      "processing: 109\n",
      "processing: 110\n",
      "processing: 111\n",
      "processing: 112\n",
      "processing: 113\n",
      "processing: 114\n",
      "processing: 115\n",
      "processing: 116\n",
      "processing: 117\n",
      "processing: 118\n",
      "processing: 119\n",
      "processing: 120\n",
      "processing: 121\n",
      "processing: 122\n",
      "processing: 123\n",
      "processing: 124\n",
      "processing: 125\n",
      "processing: 126\n",
      "processing: 127\n",
      "processing: 128\n",
      "processing: 129\n",
      "processing: 130\n",
      "processing: 131\n",
      "processing: 132\n",
      "processing: 133\n",
      "processing: 134\n",
      "processing: 135\n",
      "processing: 136\n",
      "processing: 137\n",
      "processing: 138\n",
      "processing: 139\n",
      "processing: 140\n",
      "processing: 141\n",
      "processing: 142\n",
      "processing: 143\n",
      "processing: 144\n",
      "processing: 145\n",
      "processing: 146\n",
      "processing: 147\n",
      "processing: 148\n",
      "processing: 149\n",
      "processing: 150\n",
      "processing: 151\n",
      "processing: 152\n",
      "processing: 153\n",
      "processing: 154\n",
      "processing: 155\n",
      "processing: 156\n",
      "processing: 157\n",
      "processing: 158\n",
      "processing: 159\n",
      "processing: 160\n",
      "processing: 161\n",
      "processing: 162\n",
      "processing: 163\n",
      "processing: 164\n",
      "processing: 165\n",
      "processing: 166\n",
      "processing: 167\n",
      "processing: 168\n",
      "processing: 169\n",
      "processing: 170\n",
      "processing: 171\n",
      "processing: 172\n",
      "processing: 173\n",
      "processing: 174\n",
      "processing: 175\n",
      "processing: 176\n",
      "processing: 177\n",
      "processing: 178\n"
     ]
    }
   ],
   "source": [
    "GeMAPS_sample = get_GeMAPS(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.mean(GeMAPS_sample,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                             0.000000\n",
       "Loudness_sma3                     4.690456\n",
       "alphaRatio_sma3                  -3.991197\n",
       "hammarbergIndex_sma3             15.528037\n",
       "slope0-500_sma3                  -0.073613\n",
       "slope500-1500_sma3               -0.001370\n",
       "spectralFlux_sma3                 3.028974\n",
       "mfcc1_sma3                       -6.776380\n",
       "mfcc2_sma3                        3.093613\n",
       "mfcc3_sma3                        2.390142\n",
       "mfcc4_sma3                        5.421530\n",
       "F0semitoneFrom27.5Hz_sma3nz       0.000000\n",
       "jitterLocal_sma3nz                0.000000\n",
       "shimmerLocaldB_sma3nz             0.000000\n",
       "HNRdBACF_sma3nz                   0.000000\n",
       "logRelF0-H1-H2_sma3nz             0.000000\n",
       "logRelF0-H1-A3_sma3nz             0.000000\n",
       "F1frequency_sma3nz              614.705933\n",
       "F1bandwidth_sma3nz              640.605286\n",
       "F1amplitudeLogRelF0_sma3nz     -201.000000\n",
       "F2frequency_sma3nz             1503.727295\n",
       "F2bandwidth_sma3nz              766.380249\n",
       "F2amplitudeLogRelF0_sma3nz     -201.000000\n",
       "F3frequency_sma3nz             2425.770752\n",
       "F3bandwidth_sma3nz              735.572998\n",
       "F3amplitudeLogRelF0_sma3nz     -201.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GeMAPS_0105.to_csv('/home/xuranzeng/thesis/GeMAPS_0105.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = pd.concat([sample, GeMAPS_0105], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.to_csv('/home/xuranzeng/thesis/features_0105.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/xuranzeng/thesis_git/code/data/EC_feature.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [6.287932395935059, -6.639183044433594, 18.396...\n",
       "1        [6.253244400024414, -6.663923263549805, 18.445...\n",
       "2        [6.120508193969727, -6.574212074279785, 18.131...\n",
       "3        [6.2892913818359375, -6.71579122543335, 18.561...\n",
       "4        [6.296769142150879, -6.597139358520508, 18.456...\n",
       "                               ...                        \n",
       "89838    [6.285790920257568, -6.669467926025391, 18.443...\n",
       "89839    [6.296391010284424, -6.643906116485596, 18.443...\n",
       "89840    [6.293277263641357, -6.729918003082275, 18.471...\n",
       "89841    [6.290667533874512, -6.693284034729004, 18.534...\n",
       "89842    [6.301898002624512, -6.689201354980469, 18.515...\n",
       "Name: gemaps, Length: 89843, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)['gemaps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# zip file handler  \n",
    "zip = zipfile.ZipFile('/home/xuranzeng/thesis/ReleasedDataset.zip')\n",
    "\n",
    "# list available files in the container\n",
    "filename = zip.namelist()\n",
    "\n",
    "# clean path and file names\n",
    "audioname = (item for item in filename if item.endswith('txt')) \n",
    "data = pd.DataFrame(list(audioname))\n",
    "data = pd.concat([data, data[0].str.split('/', 4, expand=True)], axis=1) # split path \n",
    "data = pd.concat([data, data[1].str.split('_', 2, expand=True)], axis=1) # split company and day\n",
    "data.columns=[\"Path\",\"Basic\", \"Company_Day\", \"Type\", \"Filename\",\"Company\",\"Day\"] # rename column\n",
    "data = data[['Path','Company',\"Day\",\"Filename\"]] # delete useless columns\n",
    "\n",
    "# path\n",
    "base_path='/home/xuranzeng/thesis/EC/'\n",
    "data['Complete_Path'] = base_path + data['Path'].astype(str)\n",
    "data = data[['Complete_Path','Company',\"Day\",\"Filename\"]] # delete useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# zip file handler  \n",
    "zip = zipfile.ZipFile('/home/xuranzeng/thesis/ReleasedDataset.zip')\n",
    "\n",
    "# list available files in the container\n",
    "filename = zip.namelist()\n",
    "\n",
    "audioname = (item for item in filename if item.endswith('txt'))\n",
    "data = pd.DataFrame(list(audioname))\n",
    "\n",
    "data = pd.concat([data, data[0].str.split('/', 2, expand=True)], axis=1) # split path \n",
    "data = pd.concat([data, data[1].str.split('_', 2, expand=True)], axis=1) # split company and day\n",
    "\n",
    "data.columns=[\"Path\",\"Basic\", \"Company_Day\", \"Filename\",\"Company\",\"Day\"] # rename column\n",
    "data = data[['Path','Company',\"Day\",\"Filename\"]] # delete useless columns\n",
    "\n",
    "# path\n",
    "base_path='/home/xuranzeng/thesis/EC/'\n",
    "data['Complete_Path'] = base_path + data['Path'].astype(str)\n",
    "data = data[['Complete_Path','Company',\"Day\",\"Filename\"]] # delete useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('/home/xuranzeng/thesis/text_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paragraph and sentence\n",
    "audio = pd.read_csv('/home/xuranzeng/thesis/EC_path.csv').iloc[: , 1:]\n",
    "audio['Day']=pd.to_numeric(audio.Day)\n",
    "audio = pd.concat([audio, audio['Filename'].str.split('_', 3, expand=True)], axis=1) # split path \n",
    "audio.columns=[\"Complete_Path\",\"Company\", \"Day\", \"Filename\",\"Speaker\",\"Paragraph\",\"Sentence\"] # rename column\n",
    "\n",
    "audio.loc[:,'Sentence']=audio.loc[:,'Sentence'].str.replace(\".mp3\", \"\")\n",
    "audio['Paragraph']=pd.to_numeric(audio.Paragraph)\n",
    "audio['Sentence']=pd.to_numeric(audio.Sentence)\n",
    "\n",
    "\n",
    "audio = audio.sort_values([\"Company\",\"Day\",\"Paragraph\", \"Sentence\"], ascending = (True,True,True,  True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio.to_csv('/home/xuranzeng/thesis/EC_path2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text audio\n",
    "\n",
    "data = pd.read_csv('/home/xuranzeng/thesis/text_path.csv').iloc[: , 1:]\n",
    "data = data[['Complete_Path','Company',\"Day\"]] # delete useless columns\n",
    "data.columns=[\"Text_Path\",\"Company\", \"Day\"] # rename column\n",
    "\n",
    "audio = pd.read_csv('/home/xuranzeng/thesis/EC_path2.csv').iloc[: , 1:-2]\n",
    "\n",
    "\n",
    "audio['Day']=pd.to_numeric(audio.Day)\n",
    "data['Day']=pd.to_numeric(data.Day)\n",
    "\n",
    "merged_path = pd.merge(audio, data, on = ['Day','Company']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_path.to_csv('/home/xuranzeng/thesis/EC_path2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pd.read_csv('/home/xuranzeng/thesis/EC_path2.csv').iloc[: , 1:]\n",
    "path['Day']=pd.to_numeric(path.Day)\n",
    "company_name = \"Booking Holdings Inc\"\n",
    "day_name = 20170509\n",
    "sample = path.loc[(path[\"Company\"]==company_name) & (path[\"Day\"]==day_name)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = path.drop_duplicates(subset=['Text_Path']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "i=6\n",
    "company_name = text_path.loc[i,'Company']\n",
    "day_name = text_path.loc[i,'Day']\n",
    "sample = path.loc[(path[\"Company\"]==company_name) & (path[\"Day\"]==day_name)].reset_index(drop=True)\n",
    "text = sample.loc[0,'Text_Path']\n",
    "with open(text) as f:\n",
    "    contents = f.read()\n",
    "    #print(contents)\n",
    "    sen = contents.split('\\n')[:-1]\n",
    "    \n",
    "    \n",
    "    if len(sen)==len(sample):\n",
    "        sample['Content']=sen\n",
    "        #print(sen)\n",
    "        \n",
    "    else:\n",
    "        print(len(sen))\n",
    "        print(len(sample))\n",
    "        #print(i)\n",
    "    #print(len(sen))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20171102\n",
      "AES Corp\n"
     ]
    }
   ],
   "source": [
    "print(day_name)\n",
    "print(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanks, Ajita',\n",
       " 'Sales for the second quarter of $738 million were 11% higher than the previous year',\n",
       " 'Net earnings in the second quarter of $92 million increased 6% from 2016. Second quarter earnings per share of $0.53 increased 8% compared with 2016. Sales in our North America segment of $471 million increased 9% compared with the second quarter of 2016. The increase in sales was primarily due to higher volumes of commercial water heaters and boilers in the U.S',\n",
       " 'and pricing actions in August 2016 related to steel cost increases and inflationary pressure on other costs',\n",
       " 'Aquasana, acquired in August of 2016, added $13 million to our North America segment sales',\n",
       " 'Rest of World segment sales of $273 million increased 14% compared with 2016. China sales increased 20% in local currency, driven by higher demand for our consumer products in the region, led by water treatment and air purification products, and pricing actions due to higher steel and other costs, which were announced earlier this year',\n",
       " 'On slide 6, North America segment earnings of $109 million were 5% higher than segment earnings in the prior year',\n",
       " 'The favorable impact from higher sales of commercial water heaters and boilers and the pricing actions in the U.S',\n",
       " 'were partially offset by significantly higher steel costs',\n",
       " 'Second quarter segment margin declined to 23.2% from 24.1% one year ago due to higher steel costs',\n",
       " 'The margin on our Aquasana products is lower than the segment average, which also contributed to the margin decline',\n",
       " 'Rest of World earnings of $32.5 million were essentially the same as one year ago',\n",
       " 'Higher China sales, including the price increase, were more than offset by higher steel costs, increased selling, general and administrative expenses and a less profitable sales mix in China',\n",
       " 'Advertising to support brand building and expansion of water treatment and air purification retail outlets in Tier 2 and 3 cities were the primary drivers of higher SG&A in China',\n",
       " 'Currency translation reduced China earnings by approximately $2 million compared with the prior year',\n",
       " 'As a result of these factors, second quarter segment margin was 11.9% compared with 13.8% last year',\n",
       " 'Our corporate expenses were essentially the same as one year ago',\n",
       " 'Our effective income tax rate in the second quarter of 2017 was 27.8%',\n",
       " 'The rate was lower than the 29.8% experienced during the second quarter last year, primarily due to lower state income taxes and a change in geographic earnings mix',\n",
       " 'The lower effective tax rate this year compared with the effective rate one year ago benefited 2017 results by $0.01 per share',\n",
       " 'Cash provided by operations during the first half of 2017 was $73 million compared with $155 million provided during the prior year',\n",
       " 'Higher earnings were more than offset by higher outlays for working capital',\n",
       " 'These factors resulted in lower cash flow in 2017. Our liquidity position and balance sheet remained strong',\n",
       " 'Our debt-to-capital ratio was 19% at the end of the second quarter',\n",
       " 'We have cash balances totaling $741 million located offshore, and our net cash position was approximately $366 million at the end of the quarter',\n",
       " 'During the first half of 2017, we repurchased approximately 1.3 million shares of common stock for a total of $66 million',\n",
       " 'Approximately 3.6 million shares remained on our existing repurchase authority at the end of June',\n",
       " 'This morning, we increased the midpoint of our 2017 EPS guidance by $0.03 per share with a range of between $2.07 and $2.11 per share',\n",
       " 'The midpoint of our EPS guidance represents a 13% increase in EPS compared with our 2016 results',\n",
       " 'Please turn to slide 9 for several 2017 assumptions',\n",
       " 'We expect our cash flow from operations in 2017 to be approximately $375 million, which is lower than the $447 million generated in 2016. We expect higher earnings in 2017, but also larger outlays for working capital due to the higher-than-anticipated cash flows in the fourth quarter of 2016. Over the two-year period from 2016 to 2017, we expect to generate operating cash of approximately $825 million, which compares with $612 million during 2014 to 2015. We broke ground in 2016 on the construction of the new water treatment and air purification manufacturing facility in Nanjing to support the strong growth of these products in China',\n",
       " 'Our 2017 capital spending plans of approximately $100 million includes $45 million related to this plant',\n",
       " 'Total costs for the facility, which is expected to begin production in the second quarter of 2018, will be about $65 million',\n",
       " 'After this expansion, we expect capital spending in 2018 and beyond to be at levels equal to our depreciation plus amortization',\n",
       " 'Our depreciation and amortization expense is expected to be approximately $70 million in 2017. As previously discussed, expenses related to our ERP implementation were $25 million in 2016 and are projected to decline to approximately $19 million in 2017. Our corporate and other expenses are expected to be approximately $48 million in 2017, slightly higher than the $45 million in 2016, primarily due to higher expenses at our Corporate Technology Center and commissioned water treatment market studies',\n",
       " 'Take note that our interest expense will be approximately $3 million higher in 2017 as a result of higher rates, share repurchase activities and our acquisition last year',\n",
       " 'Our effective income tax rate is expected to be approximately 28.5% in 2017, lower than the 29.4% rate in 2016. This assumption is predicated on no change to the current U.S',\n",
       " 'We expect to repurchase our shares in the amount of approximately $135 million in 2017 under a 10b5-1 plan',\n",
       " 'We may opportunistically repurchase additional shares up to $65 million',\n",
       " 'If $135 million of our stock is repurchased, we expect our average diluted outstanding shares in 2017 will be approximately 174.5 million',\n",
       " 'I will now turn the call back to Ajita, who will summarize our guidance, the business assumptions for 2017 and our growth strategy beginning on slide 10. Ajita',\n",
       " 'Hi, Scott',\n",
       " 'I will tell you our steel prices have increased every quarter this year',\n",
       " \"And clearly compared to the prior years, they're up significantly from the prior year\",\n",
       " 'So when we tear apart, for example, North America second quarter, the pricing could not offset the steel increases last year',\n",
       " 'I mean, the price increase last year did not offset the steel increases in the second quarter',\n",
       " 'However, as we talked about in our script, commercial water heaters was extremely strong for us in the quarter, as was commercial boilers',\n",
       " 'So, those were the contributors on why we had a net positive for the quarter',\n",
       " \"So, I don't know if that answers it, Scott, or not, but we've had continual increase in steel costs\",\n",
       " \"Well, I'll take that Scott\",\n",
       " 'When we put it in, there was an anticipation that steel prices, and we talked about it on the year-end call that we thought steel prices would level off and maybe decline',\n",
       " \"We haven't seen that at all, and they've continued to increase\",\n",
       " \"So, I think that's part of the factor on why we're seeing what we're seeing is they continued to increase higher than what we put\",\n",
       " \"Well, I'd say they weren't talked about because they were flat to the year, prior year\",\n",
       " 'And you saw that certainly in the April and May data',\n",
       " \"And I'll tell you what we saw in June, it was the same\",\n",
       " 'So, it was very flat with the prior year',\n",
       " \"And then, when you look, that's down a couple hundred thousand units from the first quarter\",\n",
       " \"But I'll tell you, overall, we're pleased where the industry is\",\n",
       " \"It's up almost 200,000 when you include tankless year-over-year\",\n",
       " 'First quarter',\n",
       " 'Well, obviously, it depends where steel goes, but we would expect it will offset']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you, Ingrid',\n",
       " 'Good morning, everyone',\n",
       " 'In Q4, Abiomed had another strong quarter of continued adoption of the Impella family of devices with $124.7 million in revenue, representing growth of 33%',\n",
       " 'patient utilization increased by 30%, driven by continued growth in both Protected PCI and emergent patients, which grew 26% and 37%, respectively',\n",
       " 'For the full fiscal year 2017, Abiomed generated $445.3 million in revenue, representing a net increase of $116 million or growth of 35%',\n",
       " 'Abiomed remains one of the fastest-growing GAAP profitable medical device companies and fiscal 2017 was another successful year, as we achieved our corporate goals',\n",
       " 'We received FDA PMA approval for cardiogenic shock, Impella CP approval for high-risk PCI, IDE approval for our STEMI Door to Unloading study, and approval in Japan',\n",
       " 'We achieved CMS approval for dedicated DRGs and expanded our clinical evidence to over 400 publications',\n",
       " 'We ramped up our distribution in the U.S',\n",
       " 'and Germany and increased manufacturing and training capacity in both Aachen, Germany and Danvers, Massachusetts to meet future demand',\n",
       " 'We also launched the Abiomed Impella Quality Program that helps improve real-world clinical outcomes with best practice protocols and education, while surpassing 50,000 patients treated in the U.S',\n",
       " 'Abiomed continues to have a solid balance sheet with a cash position of $277 million and no debt, enhancing our ability to invest in and defend our intellectual property with 274 patents and currently 241 pending',\n",
       " 'I am proud of our dedicated employees and grateful to our customers and shareholders that have enabled our company to create a new era of medicine, focused on the field of heart recovery',\n",
       " 'Fiscal 2018 is positioned to be an outstanding year',\n",
       " \"For today's call, I would like to cover two FY 2018 topics: first, growth in Protected PCI and AMI cardiogenic shock and processes to further improve clinical outcomes; second, provide a brief regulatory update and information on our Japan launch\",\n",
       " 'So, first on Protected PCI and AMI cardiogenic shock',\n",
       " 'These indications identify new treatments and therapies for cardiovascular care and heart muscle recovery represents a significant unmet clinical need for multiple growing populations',\n",
       " 'New American Heart Association statistics predict that by year 2035, 45% of the total U.S',\n",
       " 'population or approximately 131 million people will have at least one health problem related to heart disease and 24 million will have coronary artery disease',\n",
       " 'Coronary artery disease and heart failure are already the number one cause of death and cost driver in the U.S',\n",
       " 'These are massive issues for a relatively younger population that exists in every community and need solutions beyond stents, pacemakers and valves',\n",
       " 'Abiomed is uniquely qualified to help educate physicians on new treatments and therapies to enable complete revascularization and improve patient ejection fraction and quality of life by protecting or recovering heart muscle for elective, urgent or emergent patients',\n",
       " 'Regardless of the reason for the hospital admission or surgical turndown, many future heart failure patients will require hemodynamic support to stabilize and receive treatment in the cath lab, hybrid lab, EP Lab or surgery suite',\n",
       " 'Recently, in the AHA journal, Circulation, Dr',\n",
       " 'Flaherty published a 230-patient retrospective analysis titled hemodynamic support protects against acute kidney injury in patients undergoing high-risk PCI',\n",
       " 'The paper demonstrated the benefit of Impella support with significant reduction in acute kidney injury and requirement for dialysis',\n",
       " 'This new study reinforces previously published randomized clinical trial data around Protected PCI',\n",
       " \"Additionally, at the American College of Cardiology Conference, the first-ever scientific presentation of Abiomed's Impella Quality IQ Assurance Program was presented, including early data from the Detroit Cardiogenic Shock Initiative called CSI\",\n",
       " 'IQ includes nearly all of the 50,000 U.S',\n",
       " 'commercial patients in an observational database derived from those treated with Impella over the past eight years',\n",
       " 'This observational data, combined with data collected in the cVAD Registry and FDA studies, creates the largest database in the world for these high-risk patients and helps identify hospitals with the best outcomes',\n",
       " 'The IQ data indicates that top-performing hospitals achieved a 70% plus survival rate as compared to a 50% historical survival rate for AMI cardiogenic shock patients',\n",
       " 'Some best practices associated with improved outcomes in survival and heart recovery are reducing inotropic escalation using hemodynamic monitoring and initiating early implementation of Impella before PCI',\n",
       " 'The Detroit CSI initiative puts into practice these protocols',\n",
       " 'Detroit CSI is an independent collaboration between five competitive hospitals in Detroit, who are working together to demonstrate improved survival rates with heart recovery',\n",
       " 'Early data revealed survival rates of approximately 80% with all survivors discharged home with their native hearts',\n",
       " 'While these initial findings are of a small sample of approximately 40 patients, they correlate with the outcomes from top sites in the IQ Database and the cVAD Registry',\n",
       " 'These initiatives represent the new goal to achieve survival and heart recovery and emphasize the need for standardized clinically-driven protocols',\n",
       " 'Overall, we are excited to see improved outcomes at heart recovery hospitals and expect continued adoption of protocols throughout the United States, Germany, and eventually Japan',\n",
       " 'Additionally, on the CMS front, the American Hospital Association and CMS have now facilitated a system of care around the utilization of percutaneous heart pumps for Impella FDA indications for high-risk PCI, AMI cardiogenic shock, and right ventricular support',\n",
       " \"Historically, Impella was primarily reimbursed in one category, DRG 216. Over the last 18 months, the AHA Coding Clinic updated Impella to four specific categories: DRG's 216 to 221 for the cath lab, DRG 215 for care in the ICU, DRG 1 and 2 for biventricular support, and DRG's 268 and 269 for transfer of patients to specialized heart recovery hospitals\",\n",
       " \"For my second update, I'd like to highlight that this morning we announced our first patient enrolled in the STEMI DTU feasibility study\",\n",
       " 'This trial will focus on feasibility and safety and lay the groundwork for a future pivotal trial, designed to measure the impact that left ventricular unloading may have on infarct size related to reperfusion injury',\n",
       " 'We are excited to learn more about this new therapy and how it may impact a 200,000 patient per year population that is currently not treated with Impella or in cardiogenic shock',\n",
       " 'We are entering fiscal 2018 with incredible momentum',\n",
       " 'Today, we are announcing that our Impella RP PMA submission has been completed ahead of schedule',\n",
       " 'And we anticipate approval by October with a planned launch in the U.S',\n",
       " 'in the second half of our fiscal year',\n",
       " \"We're very excited to move forward under a PMA\",\n",
       " \"On Japan, we're moving our expectation of MHLW reimbursement to September\",\n",
       " 'Multiple Japanese physician societies work extensively with MHLW to define a guidance document, which outlines the appropriate hospitals and patients that can receive Impella support',\n",
       " 'This process took longer than we expected',\n",
       " 'However, we were notified last week that the guidance document has been completed and we have selected our 10 training centers',\n",
       " \"In conclusion, we're executing on our strategic plan and the Impella platform has a long runway for growth\",\n",
       " 'We are financially secure and operationally prepared to continue to create the field of heart recovery, lead in innovation and grow shareholder value',\n",
       " 'We want to thank our shareholders for their support and all our employees and customers for their hard work and dedication to our heart recovery mission',\n",
       " 'As always, we are motivated by meaningfully impacting the lives of our patients',\n",
       " 'I will now turn the call over to Mike Tomsicek, our CFO',\n",
       " 'Good morning, Raj',\n",
       " 'Sure, Raj',\n",
       " \"So, guidance is guidance and what's possible is what's possible\",\n",
       " 'This is my 52nd earnings call and the 14th time I have forecasted revenue for the year',\n",
       " \"And we've exceeded expectations in six of the last seven years\",\n",
       " \"So, just to give you an analogy of how I look at it, we're running a marathon right now at a pace and we believe that that pace will allow us to end as a top finisher\",\n",
       " 'And in any given quarter, as we discussed, we can run faster and grow by a few more million because we have so many new products and indications and potential sites',\n",
       " \"So we continue to be one of the fastest-growing GAAP profitable companies in med-tech, but we're going to maintain our focus on patient outcomes and sustainable growth\",\n",
       " \"And in the end, we'll maintain a pace and maintain our blueprint to ensure our success to $1 billion in U.S\",\n",
       " 'revenue and beyond',\n",
       " \"So, I mean, that's kind of the way we look at guidance and we're very focused again on making sure that we're growing as the universally adopted standard-of-care in the U.S\",\n",
       " ', in Germany and soon to be in Japan',\n",
       " 'Japan is the second largest med-tech market in the world',\n",
       " 'They really do not do a lot of sternotomies and avoid transplants',\n",
       " 'So heart muscle recovery is the primary focus and goal for these patients and they have a significant clinical need to do high-risk patients in both the cath lab and EP lab',\n",
       " 'Therefore, having hemodynamic support will enable a lot more treatment, enable complete revascularization and help many heart attack patients that are going into shock',\n",
       " 'That being said, Japan is a country where we want to ensure that as we start with 10 centers, we establish the protocols upfront, have the rigor in the training, so that as we expand to other sites in future years we can maintain the pace of sustainable growth with great outcomes and then continue to expand our other products and our other indications into the entire Japanese market',\n",
       " 'And to remind everyone is we do have the CP',\n",
       " \"We have the RP which we'll be submitting at some point in the future\",\n",
       " \"And then we also have the world's smallest heart pumps for chronic patients that we're working on that we'll also be bringing longer-term into the future\",\n",
       " \"So we're very focused on launching in Japan the right way, making sure that both PMDA and MHLW are pleased with the way we launch and the way we train the Japanese customers\",\n",
       " 'And we want to again ensure we get the best outcomes we can for Japan',\n",
       " \"The net effect is that in years two and three that's where we start to see the ramp and the Japanese ramp will look very similar to what we've been doing here in the U.S\",\n",
       " 'for Impella RP',\n",
       " 'Thanks, Raj',\n",
       " 'So I think a couple things on the question is we think that the best way to grow Impella is to improve outcomes for patients and that can be for enabling high-risk PCI patients to receive complete revascularization and protection from problems with acute kidney injury or keeping somebody alive after a heart attack that goes into shock and sending them home with their native heart',\n",
       " \"The strategy comes back to what we've been talking about called the hub-and-spoke execution\",\n",
       " 'And what that means is that we want specialized centers that we would consider heart recovery centers – we would like them to be the leaders in Protected PCI',\n",
       " 'We would like them to be the hub of the spoke hospitals that are treating patients, but may want to send patients for further support to their facility',\n",
       " 'And we want all the centers that take patients that are having acute myocardial infarction and potentially who can go into shock have the ability to have Impella support',\n",
       " \"So in Europe now, you see that the balloon pump is Class III for AMI cardiogenic shock, which means it's harmful\",\n",
       " \"And we've recently seen a trend in a state that in order for a hospital to be considered a trauma center to take AMI shock patients or AMI patients that they need to have some type of strategy for percutaneous hemodynamic support\",\n",
       " 'And as this progresses, our job, as a company, is to support the hospitals and to help them improve their outcomes',\n",
       " 'And what the Impella IQ Program gives us now is the ability to partner with them to look at best practices and to align the hub-and-spoke, so that the patient can be pre-treated appropriately at the right hospital',\n",
       " \"And what's exciting is that the American Hospital Association and CMS has now helped us establish basically a systematic payment structure that's based on resources to support patients\",\n",
       " \"So that's why the DRG establishment for the cath lab or the ICU support or biventricular or even transfer from one hospital to the other is very important to our overall strategy\",\n",
       " \"So it's a long answer to your question, but the net effect is that all the sites that are implementing programs are growing\",\n",
       " 'And, again, the specialized hospitals tend to do more of the Protected PCI',\n",
       " 'But by establishing a network, we can improve outcomes in communities for essentially all of these types of populations',\n",
       " \"I think the key to success again in growing that is our formula that we've talked about for a while, which is training, data and time\",\n",
       " 'And on the data, the ultimate in data is really having FDA approvals as safe and effective and then having dedicated DRGs for the treatment of these patients is very helpful',\n",
       " \"So that's standardized\",\n",
       " 'On the training, we continue to train each quarter',\n",
       " 'We had 16 publications in the quarter',\n",
       " \"Now we're over 400. We touched almost 1,000 physicians in the quarter for educational programs to symposiums\",\n",
       " 'And on ease-of-use, we continue to make changes and updates to our console and products to ensure that the nurses to the physicians find the implantation and management of an Impella patient easier every time they have an experience',\n",
       " 'Thanks, Danielle',\n",
       " 'Chris, thanks for the question',\n",
       " 'There is two things that we want to highlight',\n",
       " \"So the first is that we're very pleased that the structure has been established for the multiple types of patients that we have\",\n",
       " \"It's complex in many ways for investors to understand the DRG system, but it's even more complex for Abiomed, because, as you all know, we have multiple types of patients\",\n",
       " 'We have young kids with viruses',\n",
       " 'We have post-partum cardiomyopathy at times',\n",
       " 'We have AMI cardiogenic shock for 50-year-old dads',\n",
       " \"We've got 69-year-olds that have high-risk PCI that are turned down for surgery\",\n",
       " 'And we have patients that have to be treated both just in the cath lab to the ICU to be transferred',\n",
       " \"And so that part of it over the last 18 months has been sorted out and we're incredibly pleased with the progress we've made\",\n",
       " \"Specifically around the analysis that was done for the proposal for payment for next year for 215, what we have discovered is that it's a very small sample size that was run as a comparison\",\n",
       " 'It did not include our Impella 2.5 and CP populations',\n",
       " \"And, therefore, we're working with CMS in the comment period to understand why such a small patient population or patient sample dropped significantly and what is the potential alternatives to do that\",\n",
       " 'But what you also have to remember is we now have DRGs for just the cath lab as well as biventricular',\n",
       " \"So, some of those patients in the past would have – the biventricular patients would have been in 215 and those are now going to be moving to DRG 1 and 2. So I'm very confident what we're going to come up with is a fair plan\",\n",
       " \"And I think what's important for folks to remember is that the DRG system is designed to allow efficient hospitals to breakeven or make a little bit of money\",\n",
       " \"So the better the outcomes, the better the margin for the hospital and this is why our protocols are so important and why we're partnering with hospitals to improve on all the patient populations\",\n",
       " 'Chris, the Impella RP is a product that provides a solution that in many cases can be considered lifesaving',\n",
       " 'We have patients that are in profound shock and that needs support on the right side as well as the left side',\n",
       " 'I do believe that the opportunity is significant',\n",
       " 'And I think with the alignment of biventricular payment at the CMS level, as well as some of the clinical data that has been published or is coming out, it will demonstrate that there is a real clinical benefit for these patients',\n",
       " \"One of the other nice discoveries that we've made with the Impella RP is that the right side tends to have a thinner muscle wall and therefore is more recoverable\",\n",
       " 'So, therefore, if you can get a patient through the time of injury and risk, the right side usually recovers 100%',\n",
       " \"And that's very important to send these patients home with their native heart\",\n",
       " 'We will be launching this as a PMA, which is a lot easier than launching under a humanitarian device exemption, which requires every hospital sign in IRB and requires a lot more administrative work',\n",
       " 'So, I think, things are aligning now with both reimbursement and the regulatory approvals',\n",
       " 'And we look forward to the second half of the year going from 127 sites today to upwards of 1,000 sites in the future',\n",
       " 'Good morning, Chris',\n",
       " \"So, Chris, if I understood you right, you wanted to know – and it was stated in Mike's section that Protected PCI growth was 26% and emergent growth is 37% with overall patient growth at 30%\",\n",
       " 'Is that what you were looking for, Chris?',\n",
       " \"And that from an RP standpoint, we'll let the regulatory process and the approval take its course\",\n",
       " \"We'' see the exact timing of it\",\n",
       " \"We've guided to October, but then we'll rollout the plan for site qualifications, training and share more specific information as those results come to pass later in the year\",\n",
       " \"But we're not prepared to, sort of not knowing exactly when the approval date will be give you specifics for the year on RP\",\n",
       " \"David, we'll let Abbott update on how they are doing with their own timeline, but this is not a timeline product that has a short leg to it\",\n",
       " \"So you've got multiple tasks to complete\",\n",
       " 'You have to finish the study and then there is again multiple indications required for FDA approvals',\n",
       " \"Relative to our plan is in 2020 and beyond – we anticipate that at some point we'll have competition\",\n",
       " \"In the short-term, we don't see anything\",\n",
       " \"In the mid-term, we don't see anything that we would consider competitive from an innovative perspective\",\n",
       " \"And our real goal is to obsolete our own products by continuing to advance them, which, later this year, I'll give an outline of some of the new improvements, enhances both for ease-of-use and product performance across the portfolio from the Impella CP with the optical sensor to the ECP product, which I just had a program review in Germany last week, which is a 9 French 4-litre pump\",\n",
       " \"And, of course, we've got the Impella 55 and the Impella BTR coming, which will allow us to enter into that chronic heart failure space for Class III patients\",\n",
       " 'So, we look at ourselves as our biggest competitor',\n",
       " \"And we're going to continue to innovate\",\n",
       " 'We actually do – we do like having Abbott in the space',\n",
       " \"They're a good company that focuses on education and they continue to bring more awareness to both Protected PCI and the limitations of the intra-aortic balloon pumps\",\n",
       " \"So, we're excited for well beyond 2020.\",\n",
       " 'Thanks, David',\n",
       " \"I don't think we've given any specifics on the timing of it\",\n",
       " \"We just said it's not included in our blueprint to $1 billion 2020 plan\",\n",
       " \"But specific to the study itself and the timing is we've projected up to 18 months\",\n",
       " 'So, obviously, if we can do it quicker than 18 months for the enrollment, we will',\n",
       " 'It is difficult today for just getting IRB approved in general for all med-tech companies',\n",
       " \"I think you've seen that trends\",\n",
       " \"But once we get all 10 sites then we'll try to be moving as quick as we can\",\n",
       " 'But the component of your question that you touch on is very insightful, because this is a dramatic difference in treating patients today which their hospitals are focused on Door to Balloon time',\n",
       " \"And that balloon is the angioplasty balloon, which they've shown that if you can open up somebody's coronary artery within 90 minutes, you'll maximize or optimize the ability for these patients to stay alive\",\n",
       " 'The problem with that approach is that we are improving survival, but there is a growing epidemic of heart failure happening, even with the latest and greatest of innovation across the board',\n",
       " 'And the statistic show that, within five years of someone suffering the first heart attack, 70% have heart failure and 40% of those patients die',\n",
       " \"Now we believe part of that challenge is because we're treating the plumbing with the coronary approach with stenting-only and there is an effect that's happening with reperfusion injury in apoptosis that is creating more muscle depth\",\n",
       " \"So that's our thesis\",\n",
       " \"And, again, if we end up showing our thesis is true then you're talking about a dramatic change to now Door to Unloading for heart attack patients that today do not get Impella and are not in cardiogenic shock\",\n",
       " 'But we have a long way to go',\n",
       " 'There is a lot of risk in any study, but we believe the science of the foundation is very solid',\n",
       " \"And we're definitely excited to potentially enter into a market where we can reduce heart muscle damage in patients that are not yet in cardiogenic shock\",\n",
       " 'Thanks, David',\n",
       " 'Jayson, the quick answer is yes, because, in the U.S',\n",
       " ', you have 1,000 heart hospitals and heart hospitals have sites that have both cath lab and surgical suites and clearly needs right side support or biventricular support',\n",
       " \"There is an additional 400 hospitals that have cath lab only and we'd agree that for those places not all of them will have it\",\n",
       " 'However, there is also guidelines, Class I recommendations that for sites that are going to do PCI or accept patients that do not have a surgical suite that they need to have a hemodynamic strategy and a transfer policy',\n",
       " 'So for those sites, they likely would still want to have a biventricular approach to enable a patient to be stabilized and transferred to one of those 1,000 heart hospitals',\n",
       " \"Last call and what we generally say, it's about 10% of the sites is what we'd consider to have an enforceable standardized protocol\",\n",
       " \"And what that would mean is they've got high-risk PCI\",\n",
       " \"They've got right side support with Impella and that matches up with our RP sites\",\n",
       " 'We have other sites that might have generic protocols, but you have to also understand that some of these hospitals have different physician groups',\n",
       " 'So while they might have a protocol getting alignment with all of the different centers is somewhat challenging',\n",
       " \"And that's why the Detroit CSI initiative is so exciting because here you have competitive hospitals with competitive physician groups aligning to say that we are going to create a protocol that we believe not only gets the best survival, but gets the best heart recovery outcome for the patient\",\n",
       " 'And this is very similar to what happened years and years ago with the Door to Balloon time metric',\n",
       " 'And the Door to Balloon time metric again shows the best outcomes with opening the coronary within 90 minutes',\n",
       " \"That protocol didn't come about with a randomized study\",\n",
       " 'That came about by physician group studying and looking at real-world evidence to improve outcomes',\n",
       " 'But just like Door to Balloon time is not effective at 290 minutes, Impella is not effective when a patient has been in cardiogenic shock for two days, three days, has had three inotropes and a balloon pump and they already are in organ failure',\n",
       " \"So what we're trying to do again is systemically get folks to look at the observational data, as well as the data that's reinforced in the cVAD Registry that shows the best way to improve survival and get the outcome of heart recovery is a formula of reducing inotropic escalation, having hemodynamic monitoring and placing the Impella and stabilizing the patient before you do the PCI\",\n",
       " 'So those are keys to success',\n",
       " \"And what's nice about the Detroit initiative it's also independent\",\n",
       " \"It's a physician group\",\n",
       " \"We'll continue to work with hospitals with our IQ Program, but the Detroit initiative is driven by the leaders in the space\",\n",
       " 'So, I can formally announce today that our fiber optic sensor on the Impella CP is CE Mark approved',\n",
       " \"And we'll be launching under a limited release in Germany over the next two quarters\",\n",
       " 'And then we plan to launch in the U.S',\n",
       " 'at the very end of our year with the Impella CP optical',\n",
       " \"And to explain again, why it's so important is the optical sensor allows us to have real-time sensing of the location of the Impella pump relative to where it is against the aortic valve; is it above or below, is it in the left ventricle or above the aortic valve\",\n",
       " \"And that's really important to maximize the flow\",\n",
       " \"It's definitely an ease-of-use aspect for patients that are in the ICU. And it will also give us in the future more ways to do smart pumping, so we can actually help wean the patient off and maximize the opportunity of heart muscle recovery\",\n",
       " 'Matt, this is Mike Minogue, and just to add to that',\n",
       " 'I was in Germany last week',\n",
       " 'And in the old days, Germany was mostly just the emergency patients',\n",
       " 'And today we have emergency patients and Protected PCI',\n",
       " 'So the Protected PCI population now is 30% or more in any given quarter of our patient population in Germany, which is a really important component to our execution and training',\n",
       " 'The second is an anecdotal story',\n",
       " 'While I was there in Berlin, I met with one of the top centers',\n",
       " 'And it is now becoming more obvious to many of the folks and the physicians in heart failure community the importance of heart recovery',\n",
       " 'And I met with a physician that has been driving heart recovery and works at one of the largest and busiest LVAD centers in all of Europe',\n",
       " 'There was a patient that had AMI cardiogenic shock, was starting to have organ failure and – but the interventional cardiologist believe the person was recoverable, worked with the surgeons who agreed to give him five days to try to recover the patient',\n",
       " 'And as the story goes, I actually met the patient',\n",
       " 'His ejection fraction is now back up above 40%',\n",
       " 'He had been discharged from the hospital and is bringing a lot of attention to another alternative, another therapy for these kind of patients',\n",
       " 'And just to remind everyone, in our indication from the FDA, we are given the approval for heart recovery, which means that this is the only technology, the only type of heart pumps that exist in the world that have that exclusive indication',\n",
       " 'Thank you, everyone, for your time today',\n",
       " 'If there is any follow-up questions, please reach out to us',\n",
       " 'Have a great day']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = text_path.loc[i,'Company']\n",
    "day_name = text_path.loc[i,'Day']\n",
    "sample = path.loc[(path[\"Company\"]==company_name) & (path[\"Day\"]==day_name)].reset_index(drop=True)\n",
    "text = sample.loc[0,'Text_Path']\n",
    "with open(text) as f:\n",
    "    contents = f.read()\n",
    "    #print(contents)\n",
    "    sen = contents.split('.')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "line=[i.split('.') for i in contents.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for sublist in line:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
